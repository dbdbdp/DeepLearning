{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5, 4)\n",
      "(5, 4)\n",
      "(4,)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "T,H=5,4\n",
    "hs=np.random.randn(T,H)\n",
    "a=np.array([0.8, 0.1, 0.03, 0.05, 0.02])\n",
    "\n",
    "ar=a.reshape(5,1).repeat(4, axis=1)     # 열벡터로 만든 후에 반복함.    근데 ar=a.reshape(5,1)만 하고 hs*ar을 하면 브로드캐스팅이 되어서 계산이 똑같다.\n",
    "print(ar.shape)                         # (5,4)\n",
    "\n",
    "t=hs*ar\n",
    "print(t.shape)                          # (5,4)\n",
    "\n",
    "c=np.sum(t, axis=0)\n",
    "print(c.shape)                          # (4,)              이게 맥락 벡터가 됨."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10, 5, 4)\n",
      "(10, 4)\n"
     ]
    }
   ],
   "source": [
    "N,T,H=10,5,4\n",
    "hs=np.random.randn(N,T,H)\n",
    "a=np.random.randn(N,T)\n",
    "ar=a.reshape(N,T,1).repeat(H,axis=2)\n",
    "#ar=a.reshape(N,T,1) # 브로드캐스트를 사용하는 겨웅\n",
    "\n",
    "t=hs*ar\n",
    "print(t.shape)          #(10,5,4)\n",
    "\n",
    "c=np.sum(t,axis=1)\n",
    "print(c.shape)          #(10,4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "from common.np import *\n",
    "from common.layers import Softmax\n",
    "\n",
    "\n",
    "class WeightSum:\n",
    "    def __init__(self):\n",
    "        self.params, self.grads = [], []\n",
    "        self.cache = None\n",
    "        \n",
    "    def forward(self, hs, a):\n",
    "        N, T, H = hs.shape\n",
    "        \n",
    "        ar= a.reshape(N, T, 1)          #.repeat(H, axis=-1)\n",
    "        t = hs * ar\n",
    "        c = np.sum(t, axis=1)\n",
    "        \n",
    "        self.cache = (hs, ar)\n",
    "        return c\n",
    "    \n",
    "    def backward(self, dc):\n",
    "        hs, ar = self.cache\n",
    "        N, T, H = hs.shape\n",
    "        dt = dc.reshape(N, 1, H).repeat(T, axis=1)  # sum의 역전파 = repeat             #(N,T,H)가 됨\n",
    "        dar = dt * hs\n",
    "        dhs = dt * ar\n",
    "        da = np.sum(dar, axis=2)  # repeat노드에 대한 역전퍄 = sum              ar은 (N,T,H) 였음-->(N,T)가 됨. 왜냐하면 a가 N,T였음.\n",
    "        \n",
    "        return dhs, da"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10, 5, 4)\n",
      "(10, 5)\n",
      "(10, 5)\n"
     ]
    }
   ],
   "source": [
    "# 맥락벡터를 구하기 위한 가중치 a를 구하는 방법.\n",
    "\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "from common.layers import Softmax\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "N,T,H=10,5,4\n",
    "hs=np.random.randn(N,T,H)       #encoder의 lstm 계층의 은닉상태 벡터\n",
    "h=np.random.randn(N,H)          #decoder의 은닉층에서 나온 은닉상태 벡터\n",
    "hr=h.reshape(N,1,H).repeat(T, axis=1)           #(N,T,H)가 됨.          #hr=h.reshape(N,1,H)            #브로드 캐스트를 사용하는 경우\n",
    "\n",
    "t=hs*hr\n",
    "print(t.shape)                                  #(10,5,4)\n",
    "\n",
    "s=np.sum(t,axis=2)\n",
    "print(s.shape)                                  #(10,5)\n",
    "\n",
    "softmax=Softmax()\n",
    "a=softmax.forward(s)\n",
    "print(a.shape)                                  #(10,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# chap08/attention_layer.py\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "from common.np import *  # import numpy as np\n",
    "from common.layers import Softmax\n",
    "\n",
    "\n",
    "class AttentionWeight:\n",
    "    def __init__(self):\n",
    "        self.params, self.grads = [], []\n",
    "        self.softmax = Softmax()\n",
    "        self.cache = None\n",
    "        \n",
    "    def forward(self, hs, h):\n",
    "        N, T, H = hs.shape\n",
    "\n",
    "        hr = h.reshape(N, 1, H)#.repeat(T, axis=1)\n",
    "        t = hs * hr\n",
    "        s = np.sum(t, axis=2)\n",
    "        a = self.softmax.forward(s)\n",
    "\n",
    "        self.cache = (hs, hr)\n",
    "        return a\n",
    "\n",
    "    def backward(self, da):                             #a는 (N,T)\n",
    "        hs, hr = self.cache\n",
    "        N, T, H = hs.shape\n",
    "\n",
    "        ds = self.softmax.backward(da)                  #ds의 shape도 (N, T)\n",
    "        dt = ds.reshape(N, T, 1).repeat(H, axis=2)\n",
    "        dhs = dt * hr\n",
    "        dhr = dt * hs\n",
    "        dh = np.sum(dhr, axis=1)\n",
    "\n",
    "        return dhs, dh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# chap08/attention_layer.py\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "from common.np import *  # import numpy as np\n",
    "from common.layers import Softmax\n",
    "\n",
    "\n",
    "class Attention:\n",
    "    def __init__(self):\n",
    "        self.params, self.grads = [], []\n",
    "        self.attention_weight_layer = AttentionWeight()\n",
    "        self.weight_sum_layer = WeightSum()\n",
    "        self.attention_weight = None\n",
    "\n",
    "    def forward(self, hs, h):\n",
    "        a = self.attention_weight_layer.forward(hs, h)\n",
    "        out = self.weight_sum_layer.forward(hs, a)\n",
    "        self.attention_weight = a\n",
    "        return out\n",
    "\n",
    "    def backward(self, dout):\n",
    "        dhs0, da = self.weight_sum_layer.backward(dout)\n",
    "        dhs1, dh = self.attention_weight_layer.backward(da)\n",
    "        dhs = dhs0 + dhs1\n",
    "        return dhs, dh\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TimeAttention:\n",
    "    def __init__(self):\n",
    "        self.params, self.grads = [], []\n",
    "        self.layers = None\n",
    "        self.attention_weights = None\n",
    "\n",
    "    def forward(self, hs_enc, hs_dec):\n",
    "        N, T, H = hs_dec.shape              #T개의 h가 모임.\n",
    "        out = np.empty_like(hs_dec)\n",
    "        self.layers = []\n",
    "        self.attention_weights = []\n",
    "\n",
    "        for t in range(T):\n",
    "            layer = Attention()\n",
    "            out[:, t, :] = layer.forward(hs_enc, hs_dec[:,t,:])\n",
    "            self.layers.append(layer)\n",
    "            self.attention_weights.append(layer.attention_weight)\n",
    "\n",
    "        return out\n",
    "\n",
    "    def backward(self, dout):\n",
    "        N, T, H = dout.shape\n",
    "        dhs_enc = 0\n",
    "        dhs_dec = np.empty_like(dout)\n",
    "\n",
    "        for t in range(T):\n",
    "            layer = self.layers[t]\n",
    "            dhs, dh = layer.backward(dout[:, t, :])\n",
    "            dhs_enc += dhs                                  #repeat의 반대는 sum\n",
    "            dhs_dec[:,t,:] = dh\n",
    "\n",
    "        return dhs_enc, dhs_dec\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Attention을 적용한 seq2seq를 구현해보자\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# chap08/attention_seq2seq.py\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "from common.time_layers import *\n",
    "from seq2seq import Encoder, Seq2seq\n",
    "from attention_layer import TimeAttention\n",
    "\n",
    "\n",
    "class AttentionEncoder(Encoder):                        #기존의 Encoder를 상속.\n",
    "    def forward(self, xs):\n",
    "        xs = self.embed.forward(xs)\n",
    "        hs = self.lstm.forward(xs)\n",
    "        return hs\n",
    "\n",
    "    def backward(self, dhs):\n",
    "        dout = self.lstm.backward(dhs)\n",
    "        dout = self.embed.backward(dout)\n",
    "        return dout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttentionDecoder:\n",
    "    def __init__(self, vocab_size, wordvec_size, hidden_size):\n",
    "        V, D, H = vocab_size, wordvec_size, hidden_size\n",
    "        rn = np.random.randn\n",
    "\n",
    "        embed_W = (rn(V, D) / 100).astype('f')\n",
    "        lstm_Wx = (rn(D, 4 * H) / np.sqrt(D)).astype('f')\n",
    "        lstm_Wh = (rn(H, 4 * H) / np.sqrt(H)).astype('f')\n",
    "        lstm_b = np.zeros(4 * H).astype('f')\n",
    "        affine_W = (rn(2*H, V) / np.sqrt(2*H)).astype('f')\n",
    "        affine_b = np.zeros(V).astype('f')\n",
    "\n",
    "        self.embed = TimeEmbedding(embed_W)\n",
    "        self.lstm = TimeLSTM(lstm_Wx, lstm_Wh, lstm_b, stateful=True)\n",
    "        self.attention = TimeAttention()  # Attention 레이어 \n",
    "        self.affine = TimeAffine(affine_W, affine_b)\n",
    "        layers = [self.embed, self.lstm, self.attention, self.affine]\n",
    "\n",
    "        self.params, self.grads = [], []\n",
    "        for layer in layers:\n",
    "            self.params += layer.params\n",
    "            self.grads += layer.grads\n",
    "\n",
    "    def forward(self, xs, enc_hs):\n",
    "        h = enc_hs[:,-1]                            #hs의 마지막줄줄\n",
    "        self.lstm.set_state(h)\n",
    "\n",
    "        out = self.embed.forward(xs)\n",
    "        dec_hs = self.lstm.forward(out)\n",
    "        c = self.attention.forward(enc_hs, dec_hs)  # context vector\n",
    "        out = np.concatenate((c, dec_hs), axis=2)  # context_vector & lstm h_t\n",
    "        score = self.affine.forward(out)\n",
    "\n",
    "        return score\n",
    "\n",
    "    def backward(self, dscore):\n",
    "        dout = self.affine.backward(dscore)\n",
    "        N, T, H2 = dout.shape\n",
    "        H = H2 // 2\n",
    "\n",
    "        dc, ddec_hs0 = dout[:,:,:H], dout[:,:,H:]\n",
    "        denc_hs, ddec_hs1 = self.attention.backward(dc)\n",
    "        ddec_hs = ddec_hs0 + ddec_hs1\n",
    "        dout = self.lstm.backward(ddec_hs)\n",
    "        dh = self.lstm.dh\n",
    "        denc_hs[:, -1] += dh                                #마지막 줄에 대해서 ㅇㅇ\n",
    "        self.embed.backward(dout)\n",
    "\n",
    "        return denc_hs\n",
    "\n",
    "    def generate(self, enc_hs, start_id, sample_size):\n",
    "        sampled = []\n",
    "        sample_id = start_id\n",
    "        h = enc_hs[:, -1]\n",
    "        self.lstm.set_state(h)\n",
    "\n",
    "        for _ in range(sample_size):\n",
    "            x = np.array([sample_id]).reshape((1, 1))\n",
    "\n",
    "            out = self.embed.forward(x)\n",
    "            dec_hs = self.lstm.forward(out)\n",
    "            c = self.attention.forward(enc_hs, dec_hs)\n",
    "            out = np.concatenate((c, dec_hs), axis=2)\n",
    "            score = self.affine.forward(out)\n",
    "\n",
    "            sample_id = np.argmax(score.flatten())\n",
    "            sampled.append(sample_id)\n",
    "\n",
    "        return sampled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 6,  7,  8],\n",
       "       [15, 16, 17]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a=np.array([[[1,2,3],\n",
    "   [3,4,5],\n",
    "   [6,7,8]],\n",
    "   [[9,10,11],\n",
    "   [12,13,14],\n",
    "   [15,16,17]]])\n",
    "a.shape\n",
    "\n",
    "a[:,-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttentionSeq2seq(Seq2seq):                                    #Seq2seq상속속\n",
    "    def __init__(self, vocab_size, wordvec_size, hidden_size):\n",
    "        args = vocab_size, wordvec_size, hidden_size\n",
    "        self.encoder = AttentionEncoder(*args)\n",
    "        self.decoder = AttentionDecoder(*args)\n",
    "        self.softmax = TimeSoftmaxWithLoss()\n",
    "\n",
    "        self.params = self.encoder.params + self.decoder.params\n",
    "        self.grads = self.encoder.grads + self.decoder.grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| 에폭 1 |  반복 1 / 351 | 시간 0[s] | 손실 4.06\n",
      "| 에폭 1 |  반복 151 / 351 | 시간 35[s] | 손실 1.64\n",
      "| 에폭 1 |  반복 301 / 351 | 시간 71[s] | 손실 1.06\n",
      "Q 10/15/94                     \n",
      "T 994-10-15\n",
      "X 973-03-06\n",
      "---\n",
      "Q thursday, november 13, 2008  \n",
      "T 008-11-13\n",
      "X 004-03-06\n",
      "---\n",
      "Q Mar 25, 2003                 \n",
      "T 003-03-25\n",
      "X 004-03-06\n",
      "---\n",
      "Q Tuesday, November 22, 2016   \n",
      "T 016-11-22\n",
      "X 004-03-06\n",
      "---\n",
      "Q Saturday, July 18, 1970      \n",
      "T 970-07-18\n",
      "X 973-03-06\n",
      "---\n",
      "Q october 6, 1992              \n",
      "T 992-10-06\n",
      "X 973-03-06\n",
      "---\n",
      "Q 8/23/08                      \n",
      "T 008-08-23\n",
      "X 004-03-06\n",
      "---\n",
      "Q 8/30/07                      \n",
      "T 007-08-30\n",
      "X 004-03-06\n",
      "---\n",
      "Q 10/28/13                     \n",
      "T 013-10-28\n",
      "X 004-03-06\n",
      "---\n",
      "Q sunday, november 6, 2016     \n",
      "T 016-11-06\n",
      "X 004-03-06\n",
      "---\n",
      "정확도 0.000%\n",
      "| 에폭 2 |  반복 1 / 351 | 시간 0[s] | 손실 1.04\n",
      "| 에폭 2 |  반복 151 / 351 | 시간 35[s] | 손실 1.02\n",
      "| 에폭 2 |  반복 301 / 351 | 시간 69[s] | 손실 0.80\n",
      "Q 10/15/94                     \n",
      "T 994-10-15\n",
      "O 994-10-15\n",
      "---\n",
      "Q thursday, november 13, 2008  \n",
      "T 008-11-13\n",
      "O 008-11-13\n",
      "---\n",
      "Q Mar 25, 2003                 \n",
      "T 003-03-25\n",
      "X 003-05-25\n",
      "---\n",
      "Q Tuesday, November 22, 2016   \n",
      "T 016-11-22\n",
      "O 016-11-22\n",
      "---\n",
      "Q Saturday, July 18, 1970      \n",
      "T 970-07-18\n",
      "O 970-07-18\n",
      "---\n",
      "Q october 6, 1992              \n",
      "T 992-10-06\n",
      "O 992-10-06\n",
      "---\n",
      "Q 8/23/08                      \n",
      "T 008-08-23\n",
      "O 008-08-23\n",
      "---\n",
      "Q 8/30/07                      \n",
      "T 007-08-30\n",
      "O 007-08-30\n",
      "---\n",
      "Q 10/28/13                     \n",
      "T 013-10-28\n",
      "O 013-10-28\n",
      "---\n",
      "Q sunday, november 6, 2016     \n",
      "T 016-11-06\n",
      "O 016-11-06\n",
      "---\n",
      "정확도 84.320%\n",
      "| 에폭 3 |  반복 1 / 351 | 시간 0[s] | 손실 0.17\n",
      "| 에폭 3 |  반복 151 / 351 | 시간 34[s] | 손실 0.06\n",
      "| 에폭 3 |  반복 301 / 351 | 시간 69[s] | 손실 0.01\n",
      "Q 10/15/94                     \n",
      "T 994-10-15\n",
      "O 994-10-15\n",
      "---\n",
      "Q thursday, november 13, 2008  \n",
      "T 008-11-13\n",
      "O 008-11-13\n",
      "---\n",
      "Q Mar 25, 2003                 \n",
      "T 003-03-25\n",
      "O 003-03-25\n",
      "---\n",
      "Q Tuesday, November 22, 2016   \n",
      "T 016-11-22\n",
      "O 016-11-22\n",
      "---\n",
      "Q Saturday, July 18, 1970      \n",
      "T 970-07-18\n",
      "O 970-07-18\n",
      "---\n",
      "Q october 6, 1992              \n",
      "T 992-10-06\n",
      "O 992-10-06\n",
      "---\n",
      "Q 8/23/08                      \n",
      "T 008-08-23\n",
      "O 008-08-23\n",
      "---\n",
      "Q 8/30/07                      \n",
      "T 007-08-30\n",
      "O 007-08-30\n",
      "---\n",
      "Q 10/28/13                     \n",
      "T 013-10-28\n",
      "O 013-10-28\n",
      "---\n",
      "Q sunday, november 6, 2016     \n",
      "T 016-11-06\n",
      "O 016-11-06\n",
      "---\n",
      "정확도 99.960%\n",
      "| 에폭 4 |  반복 1 / 351 | 시간 0[s] | 손실 0.01\n",
      "| 에폭 4 |  반복 151 / 351 | 시간 35[s] | 손실 0.00\n",
      "| 에폭 4 |  반복 301 / 351 | 시간 69[s] | 손실 0.00\n",
      "Q 10/15/94                     \n",
      "T 994-10-15\n",
      "O 994-10-15\n",
      "---\n",
      "Q thursday, november 13, 2008  \n",
      "T 008-11-13\n",
      "O 008-11-13\n",
      "---\n",
      "Q Mar 25, 2003                 \n",
      "T 003-03-25\n",
      "O 003-03-25\n",
      "---\n",
      "Q Tuesday, November 22, 2016   \n",
      "T 016-11-22\n",
      "O 016-11-22\n",
      "---\n",
      "Q Saturday, July 18, 1970      \n",
      "T 970-07-18\n",
      "O 970-07-18\n",
      "---\n",
      "Q october 6, 1992              \n",
      "T 992-10-06\n",
      "O 992-10-06\n",
      "---\n",
      "Q 8/23/08                      \n",
      "T 008-08-23\n",
      "O 008-08-23\n",
      "---\n",
      "Q 8/30/07                      \n",
      "T 007-08-30\n",
      "O 007-08-30\n",
      "---\n",
      "Q 10/28/13                     \n",
      "T 013-10-28\n",
      "O 013-10-28\n",
      "---\n",
      "Q sunday, november 6, 2016     \n",
      "T 016-11-06\n",
      "O 016-11-06\n",
      "---\n",
      "정확도 99.960%\n",
      "| 에폭 5 |  반복 1 / 351 | 시간 0[s] | 손실 0.00\n",
      "| 에폭 5 |  반복 151 / 351 | 시간 33[s] | 손실 0.00\n",
      "| 에폭 5 |  반복 301 / 351 | 시간 67[s] | 손실 0.00\n",
      "Q 10/15/94                     \n",
      "T 994-10-15\n",
      "O 994-10-15\n",
      "---\n",
      "Q thursday, november 13, 2008  \n",
      "T 008-11-13\n",
      "O 008-11-13\n",
      "---\n",
      "Q Mar 25, 2003                 \n",
      "T 003-03-25\n",
      "O 003-03-25\n",
      "---\n",
      "Q Tuesday, November 22, 2016   \n",
      "T 016-11-22\n",
      "O 016-11-22\n",
      "---\n",
      "Q Saturday, July 18, 1970      \n",
      "T 970-07-18\n",
      "O 970-07-18\n",
      "---\n",
      "Q october 6, 1992              \n",
      "T 992-10-06\n",
      "O 992-10-06\n",
      "---\n",
      "Q 8/23/08                      \n",
      "T 008-08-23\n",
      "O 008-08-23\n",
      "---\n",
      "Q 8/30/07                      \n",
      "T 007-08-30\n",
      "O 007-08-30\n",
      "---\n",
      "Q 10/28/13                     \n",
      "T 013-10-28\n",
      "O 013-10-28\n",
      "---\n",
      "Q sunday, november 6, 2016     \n",
      "T 016-11-06\n",
      "O 016-11-06\n",
      "---\n",
      "정확도 100.000%\n",
      "| 에폭 6 |  반복 1 / 351 | 시간 0[s] | 손실 0.00\n",
      "| 에폭 6 |  반복 151 / 351 | 시간 34[s] | 손실 0.00\n",
      "| 에폭 6 |  반복 301 / 351 | 시간 67[s] | 손실 0.00\n",
      "Q 10/15/94                     \n",
      "T 994-10-15\n",
      "O 994-10-15\n",
      "---\n",
      "Q thursday, november 13, 2008  \n",
      "T 008-11-13\n",
      "O 008-11-13\n",
      "---\n",
      "Q Mar 25, 2003                 \n",
      "T 003-03-25\n",
      "O 003-03-25\n",
      "---\n",
      "Q Tuesday, November 22, 2016   \n",
      "T 016-11-22\n",
      "O 016-11-22\n",
      "---\n",
      "Q Saturday, July 18, 1970      \n",
      "T 970-07-18\n",
      "O 970-07-18\n",
      "---\n",
      "Q october 6, 1992              \n",
      "T 992-10-06\n",
      "O 992-10-06\n",
      "---\n",
      "Q 8/23/08                      \n",
      "T 008-08-23\n",
      "O 008-08-23\n",
      "---\n",
      "Q 8/30/07                      \n",
      "T 007-08-30\n",
      "O 007-08-30\n",
      "---\n",
      "Q 10/28/13                     \n",
      "T 013-10-28\n",
      "O 013-10-28\n",
      "---\n",
      "Q sunday, november 6, 2016     \n",
      "T 016-11-06\n",
      "O 016-11-06\n",
      "---\n",
      "정확도 100.000%\n",
      "| 에폭 7 |  반복 1 / 351 | 시간 0[s] | 손실 0.00\n",
      "| 에폭 7 |  반복 151 / 351 | 시간 33[s] | 손실 0.00\n",
      "| 에폭 7 |  반복 301 / 351 | 시간 67[s] | 손실 0.00\n",
      "Q 10/15/94                     \n",
      "T 994-10-15\n",
      "O 994-10-15\n",
      "---\n",
      "Q thursday, november 13, 2008  \n",
      "T 008-11-13\n",
      "O 008-11-13\n",
      "---\n",
      "Q Mar 25, 2003                 \n",
      "T 003-03-25\n",
      "O 003-03-25\n",
      "---\n",
      "Q Tuesday, November 22, 2016   \n",
      "T 016-11-22\n",
      "O 016-11-22\n",
      "---\n",
      "Q Saturday, July 18, 1970      \n",
      "T 970-07-18\n",
      "O 970-07-18\n",
      "---\n",
      "Q october 6, 1992              \n",
      "T 992-10-06\n",
      "O 992-10-06\n",
      "---\n",
      "Q 8/23/08                      \n",
      "T 008-08-23\n",
      "O 008-08-23\n",
      "---\n",
      "Q 8/30/07                      \n",
      "T 007-08-30\n",
      "O 007-08-30\n",
      "---\n",
      "Q 10/28/13                     \n",
      "T 013-10-28\n",
      "O 013-10-28\n",
      "---\n",
      "Q sunday, november 6, 2016     \n",
      "T 016-11-06\n",
      "O 016-11-06\n",
      "---\n",
      "정확도 100.000%\n",
      "| 에폭 8 |  반복 1 / 351 | 시간 0[s] | 손실 0.00\n",
      "| 에폭 8 |  반복 151 / 351 | 시간 34[s] | 손실 0.00\n",
      "| 에폭 8 |  반복 301 / 351 | 시간 67[s] | 손실 0.00\n",
      "Q 10/15/94                     \n",
      "T 994-10-15\n",
      "O 994-10-15\n",
      "---\n",
      "Q thursday, november 13, 2008  \n",
      "T 008-11-13\n",
      "O 008-11-13\n",
      "---\n",
      "Q Mar 25, 2003                 \n",
      "T 003-03-25\n",
      "O 003-03-25\n",
      "---\n",
      "Q Tuesday, November 22, 2016   \n",
      "T 016-11-22\n",
      "O 016-11-22\n",
      "---\n",
      "Q Saturday, July 18, 1970      \n",
      "T 970-07-18\n",
      "O 970-07-18\n",
      "---\n",
      "Q october 6, 1992              \n",
      "T 992-10-06\n",
      "O 992-10-06\n",
      "---\n",
      "Q 8/23/08                      \n",
      "T 008-08-23\n",
      "O 008-08-23\n",
      "---\n",
      "Q 8/30/07                      \n",
      "T 007-08-30\n",
      "O 007-08-30\n",
      "---\n",
      "Q 10/28/13                     \n",
      "T 013-10-28\n",
      "O 013-10-28\n",
      "---\n",
      "Q sunday, november 6, 2016     \n",
      "T 016-11-06\n",
      "O 016-11-06\n",
      "---\n",
      "정확도 100.000%\n",
      "| 에폭 9 |  반복 1 / 351 | 시간 0[s] | 손실 0.00\n",
      "| 에폭 9 |  반복 151 / 351 | 시간 38[s] | 손실 0.00\n",
      "| 에폭 9 |  반복 301 / 351 | 시간 72[s] | 손실 0.00\n",
      "Q 10/15/94                     \n",
      "T 994-10-15\n",
      "O 994-10-15\n",
      "---\n",
      "Q thursday, november 13, 2008  \n",
      "T 008-11-13\n",
      "O 008-11-13\n",
      "---\n",
      "Q Mar 25, 2003                 \n",
      "T 003-03-25\n",
      "O 003-03-25\n",
      "---\n",
      "Q Tuesday, November 22, 2016   \n",
      "T 016-11-22\n",
      "O 016-11-22\n",
      "---\n",
      "Q Saturday, July 18, 1970      \n",
      "T 970-07-18\n",
      "O 970-07-18\n",
      "---\n",
      "Q october 6, 1992              \n",
      "T 992-10-06\n",
      "O 992-10-06\n",
      "---\n",
      "Q 8/23/08                      \n",
      "T 008-08-23\n",
      "O 008-08-23\n",
      "---\n",
      "Q 8/30/07                      \n",
      "T 007-08-30\n",
      "O 007-08-30\n",
      "---\n",
      "Q 10/28/13                     \n",
      "T 013-10-28\n",
      "O 013-10-28\n",
      "---\n",
      "Q sunday, november 6, 2016     \n",
      "T 016-11-06\n",
      "O 016-11-06\n",
      "---\n",
      "정확도 100.000%\n",
      "| 에폭 10 |  반복 1 / 351 | 시간 0[s] | 손실 0.00\n",
      "| 에폭 10 |  반복 151 / 351 | 시간 33[s] | 손실 0.00\n",
      "| 에폭 10 |  반복 301 / 351 | 시간 67[s] | 손실 0.00\n",
      "Q 10/15/94                     \n",
      "T 994-10-15\n",
      "O 994-10-15\n",
      "---\n",
      "Q thursday, november 13, 2008  \n",
      "T 008-11-13\n",
      "O 008-11-13\n",
      "---\n",
      "Q Mar 25, 2003                 \n",
      "T 003-03-25\n",
      "O 003-03-25\n",
      "---\n",
      "Q Tuesday, November 22, 2016   \n",
      "T 016-11-22\n",
      "O 016-11-22\n",
      "---\n",
      "Q Saturday, July 18, 1970      \n",
      "T 970-07-18\n",
      "O 970-07-18\n",
      "---\n",
      "Q october 6, 1992              \n",
      "T 992-10-06\n",
      "O 992-10-06\n",
      "---\n",
      "Q 8/23/08                      \n",
      "T 008-08-23\n",
      "O 008-08-23\n",
      "---\n",
      "Q 8/30/07                      \n",
      "T 007-08-30\n",
      "O 007-08-30\n",
      "---\n",
      "Q 10/28/13                     \n",
      "T 013-10-28\n",
      "O 013-10-28\n",
      "---\n",
      "Q sunday, november 6, 2016     \n",
      "T 016-11-06\n",
      "O 016-11-06\n",
      "---\n",
      "정확도 100.000%\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "import matplotlib.font_manager as fm\n",
    "font_path = 'C:/Windows/Fonts/malgun.ttf'\n",
    "font_name = fm.FontProperties(fname=font_path, size=10).get_name()\n",
    "plt.rc('font', family=font_name, size=12)\n",
    "from dataset import sequence\n",
    "from common.optimizer import Adam\n",
    "from common.trainer import Trainer\n",
    "from common.util import eval_seq2seq\n",
    "from attention_seq2seq import AttentionSeq2seq\n",
    "from seq2seq import Seq2seq\n",
    "from peeky_seq2seq import PeekySeq2seq\n",
    "\n",
    "\n",
    "# 데이터 읽기\n",
    "(x_train, t_train), (x_test, t_test) = sequence.load_data('date.txt')\n",
    "char_to_id, id_to_char = sequence.get_vocab()\n",
    "\n",
    "# 입력 문장 반전\n",
    "x_train, x_test = x_train[:, ::-1], x_test[:, ::-1]\n",
    "\n",
    "# 하이퍼파라미터 설정\n",
    "vocab_size = len(char_to_id)\n",
    "wordvec_size = 16\n",
    "hidden_size = 256\n",
    "batch_size = 128\n",
    "max_epoch = 10\n",
    "max_grad = 5.0\n",
    "\n",
    "model = AttentionSeq2seq(vocab_size, wordvec_size, hidden_size)\n",
    "# model = Seq2seq(vocab_size, wordvec_size, hidden_size)\n",
    "# model = PeekySeq2seq(vocab_size, wordvec_size, hidden_size)\n",
    "\n",
    "optimizer = Adam()\n",
    "trainer = Trainer(model, optimizer)\n",
    "\n",
    "acc_list = []\n",
    "for epoch in range(max_epoch):\n",
    "    trainer.fit(x_train, t_train, max_epoch=1,\n",
    "                batch_size=batch_size, max_grad=max_grad, eval_interval=150)\n",
    "\n",
    "    correct_num = 0\n",
    "    for i in range(len(x_test)):\n",
    "        question, correct = x_test[[i]], t_test[[i]]\n",
    "        verbose = i < 10\n",
    "        correct_num += eval_seq2seq(model, question, correct,\n",
    "                                    id_to_char, verbose, is_reverse=True)\n",
    "\n",
    "    acc = float(correct_num) / len(x_test)\n",
    "    acc_list.append(acc)\n",
    "    print('정확도 %.3f%%' % (acc * 100))\n",
    "\n",
    "\n",
    "model.save_params()\n",
    "\n",
    "# 그래프 그리기\n",
    "x = np.arange(len(acc_list))\n",
    "plt.plot(x, acc_list, marker='o')\n",
    "plt.xlabel('에폭')\n",
    "plt.ylabel('정확도')\n",
    "plt.ylim(-0.05, 1.05)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "exit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
