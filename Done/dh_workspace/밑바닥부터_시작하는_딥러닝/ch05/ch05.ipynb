{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 곱셈계층"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MulLayer:\n",
    "    def __init__(self):\n",
    "        self.x=None\n",
    "        self.y=None\n",
    "\n",
    "    def forward(self,x:float, y:float):\n",
    "        self.x=x\n",
    "        self.y=y\n",
    "        out=self.x*self.y\n",
    "        return out\n",
    "\n",
    "    def backward(self,dout):\n",
    "        dx=dout*self.y\n",
    "        dy=dout*self.x\n",
    "        return dx, dy                   #튜플로 나옴.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "220.00000000000003\n"
     ]
    }
   ],
   "source": [
    "# MulLayer 적용\n",
    "\n",
    "apple=100\n",
    "apple_num=2\n",
    "tax=1.1\n",
    "\n",
    "node1=MulLayer()\n",
    "node2=MulLayer()\n",
    "\n",
    "#순전파\n",
    "apple_price=node1.forward(apple,apple_num)\n",
    "price=node2.forward(apple_price,tax)\n",
    "\n",
    "print(price)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.2 110.00000000000001 200\n"
     ]
    }
   ],
   "source": [
    "#역전파\n",
    "dprice=1\n",
    "dapp,dtax=node2.backward(dprice)\n",
    "dapple,dapple_num=node1.backward(dapp)\n",
    "\n",
    "print(dapple, dapple_num,dtax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 덧셈 계층"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AddLayer:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def forward(self,x,y):\n",
    "        out=x+y\n",
    "        return out\n",
    "    \n",
    "    def backward(self,dout):\n",
    "        dx=dout*1\n",
    "        dy=dout*1\n",
    "        return dx, dy\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "715.0000000000001\n",
      "110.00000000000001 2.2 165.0 3.3000000000000003 650\n"
     ]
    }
   ],
   "source": [
    "apple_num=2\n",
    "apple=100\n",
    "tangerine_num=3\n",
    "tangerine=150\n",
    "tax=1.1\n",
    "\n",
    "node_app=MulLayer()\n",
    "node_tang=MulLayer()\n",
    "node_sum=AddLayer()\n",
    "node_price=MulLayer()\n",
    "\n",
    "#순전파\n",
    "apple_price=node_app.forward(apple_num,apple)\n",
    "tangerine_price=node_tang.forward(tangerine_num,tangerine)\n",
    "summed_price=node_sum.forward(apple_price,tangerine_price)\n",
    "price=node_price.forward(summed_price,tax)\n",
    "\n",
    "print(price)\n",
    "\n",
    "\n",
    "#역전파\n",
    "dprice=1\n",
    "dsummed,dtax =node_price.backward(dprice)\n",
    "dapp,dtang=node_sum.backward(dsummed)\n",
    "dapple_num,dapple=node_app.backward(dapp)\n",
    "dtangerine_num,dtangerine=node_tang.backward(dtang)\n",
    "print(dapple_num,dapple,dtangerine_num,dtangerine,dtax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1.  -0.5]\n",
      " [-2.   3. ]]\n",
      "\n",
      "[[False  True]\n",
      " [ True False]]\n",
      "\n",
      "[-0.5 -2. ]\n",
      "\n",
      "[[1. 0.]\n",
      " [0. 3.]]\n"
     ]
    }
   ],
   "source": [
    "# Relu 클래스를 만들기 전에 참고할 코드\n",
    "import numpy as np\n",
    "\n",
    "x=np.array([[1.0,-0.5],[-2.0,3.0]])\n",
    "print(x)\n",
    "\n",
    "print()\n",
    "\n",
    "mask=(x<=0)                     # 어레이에 조건을 적용하면 불리언으로 어레이가 만들어진다.\n",
    "print(mask)\n",
    "out=x.copy()\n",
    "\n",
    "print()\n",
    "\n",
    "print(out[mask])                # 조건이 참인 것만 추출\n",
    "\n",
    "print()\n",
    "\n",
    "out[mask]=0                     # 조건이 참인 것들에 0을 대입입\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Relu:\n",
    "    def __init__(self):\n",
    "        self.mask=None\n",
    "    \n",
    "    def forward(self,x):                # x가 array로 들어온다.\n",
    "        self.mask=(x<=0)                # self.mask를 boolean type으로 변경\n",
    "        out=x.copy()                    # x를 out에 복사를 하고\n",
    "        out[self.mask]=0                # out에 조건을 적용\n",
    "\n",
    "        return out\n",
    "    \n",
    "    def backward(self,dout):            \n",
    "        dout[self.mask]=0\n",
    "        dx=dout\n",
    "\n",
    "        return dx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.5.2 sigmoid 계층"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sigmoid:\n",
    "    def __init__(self):\n",
    "        self.out=None\n",
    "    \n",
    "    def forward(self,x):\n",
    "        out=1/(1+np.exp(-x))\n",
    "        self.out=out\n",
    "        return out\n",
    "    \n",
    "    def backward(self,dout):\n",
    "        dx=dout*(1-self.out)*self.out\n",
    "        return dx\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5.6 Affine/Softmax 계층 구현하기\n",
    "\n",
    "## 5.6.1 Affine 계층"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "class Affine:\n",
    "    def __init__(self, W, b):\n",
    "        self.W=W\n",
    "        self.b=b\n",
    "        self.x=None\n",
    "        self.dW=None\n",
    "        self.db=None\n",
    "\n",
    "    def forward(self,x):\n",
    "        self.x=x\n",
    "        out=np.dot(x,self.W)+self.b\n",
    "        return out\n",
    "    \n",
    "    def backward(self,dout):\n",
    "        dx=np.dot(dout,self.W.T)                        \n",
    "        self.dW=np.dot(self.x.T,dout)                   \n",
    "        self.db=np.sum(dout, axis=0)                    # 열끼리 더해준다.\n",
    "\n",
    "        return dx                                       # dx를 return해준다. 왜? \n",
    "                                                        # 입력 x에 대한 기울기를 이전 레이어로 전달하기 위함\n",
    "                                                        # 각 레이어에서 출력에 대한 기울기를 계산한 후, 그 기울기를 입력으로 다시 전달해야 신경망이 제대로 학습\n",
    "\n",
    "\n",
    "                                                        #그리고 self.dW와 self.db는 가중치와 편향에 대한 기울기로 나중에 손실함수의 최소화를 위해 필요하다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.6.3 Softmax-with-Loss 계층\n",
    "\n",
    "마지막으로 출력층에서 사용하는 소프트맥스 함수에 대해 설명하겠다.\n",
    "\n",
    "소프트맥스 함수는 입력값을 정규화하여 출력한다.\n",
    "\n",
    "(정규화 (Normalization): 이후 모든 클래스에 대해 지수 값을 합산하여, 각 클래스의 지수 값을 합산한 값으로 나눕니다. 이렇게 하면 출력 값들의 총합이 1이 됩니다. 이 과정은 확률 분포를 만들기 위한 정규화입니다.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#아래의 softmax함수를 위해 들고왔다.\n",
    "def Softmax(a):\n",
    "    c=np.max(a)\n",
    "    exp_a=np.exp(a-c)\n",
    "    exp_sum_a=np.sum(exp_a)\n",
    "    y=exp_a/exp_sum_a\n",
    "    \n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#아래의 cross_entropy_error함수를 위해 들고왔다.\n",
    "def cross_entropy_error(y,t):\n",
    "    if y.ndim==1:\n",
    "        t=t.reshape(1,t.size)\n",
    "        y=y.reshape(1,y.size)\n",
    "\n",
    "    batch_size=y.shape[0]\n",
    "    return -np.sum(np.log(y[np.arange(batch_size),t]+1e-7))/batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SoftmaxWithLoss:\n",
    "    def __init__(self):\n",
    "        self.loss=None\n",
    "        self.y=None\n",
    "        self.t=None\n",
    "    \n",
    "    def forward(self,x,t):\n",
    "        self.t=t\n",
    "        self.y=Softmax(x)\n",
    "        self.loss=cross_entropy_error(self.y,self.t)\n",
    "        return self.loss\n",
    "    \n",
    "    def backward(self,dout=1):\n",
    "        batch_size=self.t.shape[0]\n",
    "        dx=(self.y-self.t)/batch_size\n",
    "        return dx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5.7 오차역전파법 구하기\n",
    "\n",
    "## 5.7.1 신경망 학습의 전체 그림\n",
    "\n",
    "## 5.7.2 오차역전파법을 적용한 신경망 구현하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TwoLayerNet을 위한 함수\n",
    "\n",
    "class Affine:\n",
    "    def __init__(self, W, b):\n",
    "        self.W = W\n",
    "        self.b = b\n",
    "        \n",
    "        self.x = None\n",
    "        self.original_x_shape = None\n",
    "        # 가중치와 편향 매개변수의 미분\n",
    "        self.dW = None\n",
    "        self.db = None\n",
    "\n",
    "    def forward(self, x):\n",
    "        # 텐서 대응\n",
    "        self.original_x_shape = x.shape             \n",
    "        x = x.reshape(x.shape[0], -1)\n",
    "        self.x = x\n",
    "\n",
    "        out = np.dot(self.x, self.W) + self.b\n",
    "\n",
    "        return out\n",
    "\n",
    "    def backward(self, dout):\n",
    "        dx = np.dot(dout, self.W.T)\n",
    "        self.dW = np.dot(self.x.T, dout)\n",
    "        self.db = np.sum(dout, axis=0)\n",
    "        \n",
    "        dx = dx.reshape(*self.original_x_shape)  # 입력 데이터 모양 변경(텐서 대응)\n",
    "        return dx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TwoLayerNet을 위한 함수\n",
    "\n",
    "class Relu:\n",
    "    def __init__(self):\n",
    "        self.mask = None\n",
    "\n",
    "    def forward(self, x):\n",
    "        self.mask = (x <= 0)\n",
    "        out = x.copy()\n",
    "        out[self.mask] = 0\n",
    "\n",
    "        return out\n",
    "\n",
    "    def backward(self, dout):\n",
    "        dout[self.mask] = 0\n",
    "        dx = dout\n",
    "\n",
    "        return dx\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "class Sigmoid:\n",
    "    def __init__(self):\n",
    "        self.out = None\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = sigmoid(x)\n",
    "        self.out = out\n",
    "        return out\n",
    "\n",
    "    def backward(self, dout):\n",
    "        dx = dout * (1.0 - self.out) * self.out\n",
    "\n",
    "        return dx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### np.nditer\n",
    "\n",
    "flags=['multi_index']:\n",
    "\n",
    "* 이 플래그는 각 요소의 인덱스를 추적할 수 있게 합니다.\n",
    "* 반복할 때 multi_index를 사용하면 각 요소의 다차원 인덱스를 it.multi_index 속성으로 가져올 수 있습니다.\n",
    "\n",
    "op_flags=['readwrite']:\n",
    "\n",
    "* 이 플래그는 배열에 대한 읽기 및 쓰기 권한을 부여합니다.\n",
    "* readwrite 플래그가 지정되면 배열의 각 요소를 읽을 뿐만 아니라 수정할 수도 있습니다. 이 플래그가 없다면 해당 배열은 읽기 전용입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TwoLayerNet을 위한 함수\n",
    "\n",
    "def numerical_gradient(f, x):\n",
    "    h = 1e-4 # 0.0001\n",
    "    grad = np.zeros_like(x)\n",
    "    \n",
    "    it = np.nditer(x, flags=['multi_index'], op_flags=['readwrite'])        #배열을 이터레이터로 사용 가능\n",
    "    while not it.finished:\n",
    "        idx = it.multi_index                    #현재 원소의 다차원 인덱스\n",
    "        tmp_val = x[idx]\n",
    "        x[idx] = float(tmp_val) + h\n",
    "        fxh1 = f(x) # f(x+h)\n",
    "        \n",
    "        x[idx] = tmp_val - h \n",
    "        fxh2 = f(x) # f(x-h)\n",
    "        grad[idx] = (fxh1 - fxh2) / (2*h)\n",
    "        \n",
    "        x[idx] = tmp_val # 값 복원\n",
    "        it.iternext()   \n",
    "        \n",
    "    return grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TwoLayerNet\n",
    "import sys, os\n",
    "sys.path.append(os.pardir)\n",
    "import numpy as np\n",
    "from common.layers import *\n",
    "from common.gradient import numerical_gradient\n",
    "from collections import OrderedDict\n",
    "\n",
    "\n",
    "class TwoLayerNet:\n",
    "\n",
    "    def __init__(self, input_size, hidden_size, output_size, weight_init_std = 0.01):\n",
    "        # 가중치 초기화\n",
    "        self.params = {}\n",
    "        self.params['W1'] = weight_init_std * np.random.randn(input_size, hidden_size)\n",
    "        self.params['b1'] = np.zeros(hidden_size)\n",
    "        self.params['W2'] = weight_init_std * np.random.randn(hidden_size, output_size) \n",
    "        self.params['b2'] = np.zeros(output_size)\n",
    "\n",
    "        # 계층 생성\n",
    "        self.layers = OrderedDict()\n",
    "        self.layers['Affine1'] = Affine(self.params['W1'], self.params['b1'])\n",
    "        self.layers['Relu1'] = Relu()\n",
    "        self.layers['Affine2'] = Affine(self.params['W2'], self.params['b2'])\n",
    "\n",
    "        self.lastLayer = SoftmaxWithLoss()\n",
    "        \n",
    "    def predict(self, x):\n",
    "        for layer in self.layers.values():\n",
    "            x = layer.forward(x)\n",
    "        \n",
    "        return x\n",
    "        \n",
    "    # x : 입력 데이터, t : 정답 레이블\n",
    "    def loss(self, x, t):\n",
    "        y = self.predict(x)\n",
    "        return self.lastLayer.forward(y, t)\n",
    "    \n",
    "    def accuracy(self, x, t):\n",
    "        y = self.predict(x)\n",
    "        y = np.argmax(y, axis=1)                                        #인덱스를 반환\n",
    "        if t.ndim != 1 : t = np.argmax(t, axis=1)                       #인덱스를 반환\n",
    "        \n",
    "        accuracy = np.sum(y == t) / float(x.shape[0])\n",
    "        return accuracy\n",
    "        \n",
    "    # x : 입력 데이터, t : 정답 레이블\n",
    "    def numerical_gradient(self, x, t):                                 #수치적 미분을 이용\n",
    "        loss_W = lambda W: self.loss(x, t)\n",
    "        \n",
    "        grads = {}\n",
    "        grads['W1'] = numerical_gradient(loss_W, self.params['W1'])\n",
    "        grads['b1'] = numerical_gradient(loss_W, self.params['b1'])\n",
    "        grads['W2'] = numerical_gradient(loss_W, self.params['W2'])\n",
    "        grads['b2'] = numerical_gradient(loss_W, self.params['b2'])\n",
    "        \n",
    "        return grads\n",
    "        \n",
    "    def gradient(self, x, t):                                           #back propagation을 이용.\n",
    "        # forward\n",
    "        self.loss(x, t)\n",
    "\n",
    "        # backward\n",
    "        dout = 1\n",
    "        dout = self.lastLayer.backward(dout)\n",
    "        \n",
    "        layers = list(self.layers.values())                             #forward한 결과들이 리스트에 저장된다. Affine1->Relu1->Affine2 이런 순으로 저장된다.\n",
    "        layers.reverse()                                                #Affine2->Relu1->Affine1 이렇게 리스트가 바뀐다.\n",
    "        for layer in layers:\n",
    "            dout = layer.backward(dout)                                 #초반에 Affine2에 dout이 1들어감. \n",
    "                                                                        #return값들을 dout에 저장해서 연쇄적으로 계산가능.\n",
    "\n",
    "        # 결과 저장\n",
    "        grads = {}\n",
    "        grads['W1'], grads['b1'] = self.layers['Affine1'].dW, self.layers['Affine1'].db     #backward함수에 dw와 db를 저장했었는데 그걸 grads의 'W1'과' 'b1'에 저장\n",
    "        grads['W2'], grads['b2'] = self.layers['Affine2'].dW, self.layers['Affine2'].db     #backward함수에 dw와 db를 저장했었는데 그걸 grads의 'W2'과' 'b2'에 저장\n",
    "\n",
    "        return grads"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "오차역전파법이 더 빠르다. 앞으로 이것만을 사용하자.\n",
    "\n",
    "근데 오차역전파법이 단점은 복잡해서 종종 실수를 하는데, 제대로 구현했는지 검증하고 싶다면 두 방식으로 구한 기울기가 일치함을 확인하는 작업을 기울기 확인(GRADIENT CHECK)라고 한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W1:4.2993767776615225e-10\n",
      "b1:2.601498037051132e-09\n",
      "W2:5.072369345495358e-09\n",
      "b2:1.4031024525157366e-07\n"
     ]
    }
   ],
   "source": [
    "# coding: utf-8\n",
    "import sys, os\n",
    "sys.path.append(os.pardir)\n",
    "import numpy as np\n",
    "from dataset.mnist import load_mnist\n",
    "from two_layer_net import TwoLayerNet\n",
    "\n",
    "# 데이터 읽기\n",
    "(x_train, t_train), (x_test, t_test) = load_mnist(normalize=True, one_hot_label=True)\n",
    "\n",
    "network = TwoLayerNet(input_size=784, hidden_size=50, output_size=10)\n",
    "\n",
    "x_batch = x_train[:3]                   # 첫 세개의 데이터를 선택\n",
    "t_batch = t_train[:3]\n",
    "\n",
    "grad_numerical = network.numerical_gradient(x_batch, t_batch)\n",
    "grad_backprop = network.gradient(x_batch, t_batch)\n",
    "\n",
    "# 각 가중치의 절대 오차의 평균을 구한다.\n",
    "for key in grad_numerical.keys():\n",
    "    diff = np.average( np.abs(grad_backprop[key] - grad_numerical[key]) )\n",
    "    print(key + \":\" + str(diff))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.095 0.0961\n",
      "0.9043666666666667 0.9074\n",
      "0.9261833333333334 0.927\n",
      "0.9380166666666667 0.9369\n",
      "0.9457333333333333 0.945\n",
      "0.9513333333333334 0.9501\n",
      "0.9582833333333334 0.9528\n",
      "0.9627666666666667 0.9597\n",
      "0.9651333333333333 0.9599\n",
      "0.96875 0.9628\n",
      "0.97115 0.9641\n",
      "0.9727833333333333 0.966\n",
      "0.9735833333333334 0.9671\n",
      "0.9753666666666667 0.9688\n",
      "0.9771166666666666 0.9697\n",
      "0.9788833333333333 0.9698\n",
      "0.97965 0.9717\n"
     ]
    }
   ],
   "source": [
    "#train_neuralnet.py\n",
    "\n",
    "# coding: utf-8\n",
    "import sys, os\n",
    "sys.path.append(os.pardir)\n",
    "\n",
    "import numpy as np\n",
    "from dataset.mnist import load_mnist\n",
    "from two_layer_net import TwoLayerNet\n",
    "\n",
    "# 데이터 읽기\n",
    "(x_train, t_train), (x_test, t_test) = load_mnist(normalize=True, one_hot_label=True)\n",
    "\n",
    "network = TwoLayerNet(input_size=784, hidden_size=50, output_size=10)\n",
    "\n",
    "iters_num = 10000\n",
    "train_size = x_train.shape[0]\n",
    "batch_size = 100\n",
    "learning_rate = 0.1\n",
    "\n",
    "train_loss_list = []\n",
    "train_acc_list = []\n",
    "test_acc_list = []\n",
    "\n",
    "iter_per_epoch = max(train_size / batch_size, 1)            # 일반성을 고려한 방어적 코드. 한 번의 에폭(epoch)에서 최소 1번 이상의 학습을 보장하기 위해서입니다.\n",
    "                                                            # train_size / batch_size가 1보다 작을 수 있는 상황을 생각해보겠습니다. \n",
    "                                                            # 예를 들어, train_size = 10이고 batch_size = 20이면 train_size / batch_size = 0.5가 되어, \n",
    "                                                            # 한 에폭을 학습하는 데 필요한 배치의 수가 0.5가 됩니다. \n",
    "                                                            # 하지만 배치의 수는 1개 이상이어야 하므로 0.5는 의미가 없습니다.\n",
    "\n",
    "for i in range(iters_num):\n",
    "    batch_mask = np.random.choice(train_size, batch_size)\n",
    "    x_batch = x_train[batch_mask]\n",
    "    t_batch = t_train[batch_mask]\n",
    "    \n",
    "    # 기울기 계산\n",
    "    #grad = network.numerical_gradient(x_batch, t_batch) # 수치 미분 방식\n",
    "    grad = network.gradient(x_batch, t_batch) # 오차역전파법 방식(훨씬 빠르다)\n",
    "    \n",
    "    # 갱신\n",
    "    for key in ('W1', 'b1', 'W2', 'b2'):\n",
    "        network.params[key] -= learning_rate * grad[key]\n",
    "    \n",
    "    loss = network.loss(x_batch, t_batch)                   #loss함수의 값들을 gradient에서 구했지만 저장을 안 했으니 다시 구해주고\n",
    "    train_loss_list.append(loss)                            #그 값들을 저장해준다.\n",
    "    \n",
    "    if i % iter_per_epoch == 0:\n",
    "        train_acc = network.accuracy(x_train, t_train)\n",
    "        test_acc = network.accuracy(x_test, t_test)\n",
    "        train_acc_list.append(train_acc)\n",
    "        test_acc_list.append(test_acc)\n",
    "        print(train_acc, test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "notebook_kernel",
   "language": "python",
   "name": "notebook_kernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
