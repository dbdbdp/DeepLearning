{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class MultivariateLinearRegressionModel(nn.Module):     #nn.Module을 상속해서 인공신경망을 만드는 것을 피하게 할 수 있다.\n",
    "    def __init__(self):\n",
    "        super().__init__()              #super().__init__()이라는 코드가 다른 클래스의 속성 및 메소드를 자동으로 불러와 해당 클래스에서도 사용이 가능하도록 해준다.\n",
    "                                        #super()는 부모 클래스의 인스턴스를 반환하는 함수. __init__()는 부모 클래스의 초기화 메서드.\n",
    "        self.linear=nn.Linear(3,1)      #선형 계층(Linear Layer)을 정의. 입력차원은 3, 출력차원은 1\n",
    "                                        #self.linear는 이 선형 계층을 MultivariateLinearRegressionModel 클래스의 속성으로 저장        \n",
    "                                        #nn.Linear는 in_features(입력 특성의 수)와 out_features(출력 특성의 수)에 맞춰 W와 b를 자동으로 초기화\n",
    "\n",
    "    def forward(self,x):\n",
    "        return self.linear(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.functional import F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# minibatch 를 이용한 gradient descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pytorch Dataset\n",
    "# 여기서 x와 y에 대해 정의를 했음.\n",
    "import torch                                          \n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self):\n",
    "        self.x_data=torch.FloatTensor([[73, 80, 75],\n",
    "                           [93,88,93],\n",
    "                           [89,91,90],\n",
    "                           [96,98,100],\n",
    "                           [73,66,70]])\n",
    "        \n",
    "        self.y_data=torch.FloatTensor([[152],[185],[180],[196],[142]])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.x_data)\n",
    "    \n",
    "    def __getitem__(self,idx):\n",
    "        x=torch.FloatTensor(self.x_data[idx])\n",
    "        y=torch.FloatTensor(self.y_data[idx])\n",
    "\n",
    "        return x,y\n",
    "\n",
    "dataset=CustomDataset()\n",
    "model=MultivariateLinearRegressionModel()\n",
    "optimizer=torch.optim.SGD(model.parameters(),lr=1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#이걸 이제 mini batch 처리를 함.\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "dataloader=DataLoader(\n",
    "    dataset,\n",
    "    batch_size=2,               #보통 batch_size는 2의 제곱수로 둔다.\n",
    "    shuffle=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch    0/20 batch: 1/3, cost: 17146.664062\n",
      "epoch    0/20 batch: 2/3, cost: 12955.142578\n",
      "epoch    0/20 batch: 3/3, cost: 2748.992188\n",
      "epoch    1/20 batch: 1/3, cost: 457.923676\n",
      "epoch    1/20 batch: 2/3, cost: 306.978516\n",
      "epoch    1/20 batch: 3/3, cost: 73.785416\n",
      "epoch    2/20 batch: 1/3, cost: 16.777370\n",
      "epoch    2/20 batch: 2/3, cost: 2.841071\n",
      "epoch    2/20 batch: 3/3, cost: 4.225943\n",
      "epoch    3/20 batch: 1/3, cost: 0.900907\n",
      "epoch    3/20 batch: 2/3, cost: 0.137768\n",
      "epoch    3/20 batch: 3/3, cost: 0.111179\n",
      "epoch    4/20 batch: 1/3, cost: 0.152790\n",
      "epoch    4/20 batch: 2/3, cost: 0.625106\n",
      "epoch    4/20 batch: 3/3, cost: 0.045909\n",
      "epoch    5/20 batch: 1/3, cost: 0.391222\n",
      "epoch    5/20 batch: 2/3, cost: 0.385556\n",
      "epoch    5/20 batch: 3/3, cost: 0.230323\n",
      "epoch    6/20 batch: 1/3, cost: 0.117620\n",
      "epoch    6/20 batch: 2/3, cost: 0.098170\n",
      "epoch    6/20 batch: 3/3, cost: 1.255620\n",
      "epoch    7/20 batch: 1/3, cost: 0.246526\n",
      "epoch    7/20 batch: 2/3, cost: 0.234694\n",
      "epoch    7/20 batch: 3/3, cost: 1.045645\n",
      "epoch    8/20 batch: 1/3, cost: 0.248556\n",
      "epoch    8/20 batch: 2/3, cost: 0.374310\n",
      "epoch    8/20 batch: 3/3, cost: 0.622571\n",
      "epoch    9/20 batch: 1/3, cost: 0.133015\n",
      "epoch    9/20 batch: 2/3, cost: 0.735101\n",
      "epoch    9/20 batch: 3/3, cost: 0.274882\n",
      "epoch   10/20 batch: 1/3, cost: 0.102480\n",
      "epoch   10/20 batch: 2/3, cost: 0.630034\n",
      "epoch   10/20 batch: 3/3, cost: 0.214593\n",
      "epoch   11/20 batch: 1/3, cost: 0.658767\n",
      "epoch   11/20 batch: 2/3, cost: 0.039317\n",
      "epoch   11/20 batch: 3/3, cost: 0.407824\n",
      "epoch   12/20 batch: 1/3, cost: 0.107814\n",
      "epoch   12/20 batch: 2/3, cost: 0.719261\n",
      "epoch   12/20 batch: 3/3, cost: 0.042477\n",
      "epoch   13/20 batch: 1/3, cost: 0.199216\n",
      "epoch   13/20 batch: 2/3, cost: 0.567488\n",
      "epoch   13/20 batch: 3/3, cost: 0.363734\n",
      "epoch   14/20 batch: 1/3, cost: 0.098406\n",
      "epoch   14/20 batch: 2/3, cost: 0.667732\n",
      "epoch   14/20 batch: 3/3, cost: 0.023742\n",
      "epoch   15/20 batch: 1/3, cost: 0.309689\n",
      "epoch   15/20 batch: 2/3, cost: 0.735145\n",
      "epoch   15/20 batch: 3/3, cost: 0.023042\n",
      "epoch   16/20 batch: 1/3, cost: 0.346995\n",
      "epoch   16/20 batch: 2/3, cost: 0.418585\n",
      "epoch   16/20 batch: 3/3, cost: 0.249878\n",
      "epoch   17/20 batch: 1/3, cost: 0.657890\n",
      "epoch   17/20 batch: 2/3, cost: 0.163904\n",
      "epoch   17/20 batch: 3/3, cost: 0.346872\n",
      "epoch   18/20 batch: 1/3, cost: 0.762603\n",
      "epoch   18/20 batch: 2/3, cost: 0.099135\n",
      "epoch   18/20 batch: 3/3, cost: 0.067780\n",
      "epoch   19/20 batch: 1/3, cost: 0.489049\n",
      "epoch   19/20 batch: 2/3, cost: 0.229220\n",
      "epoch   19/20 batch: 3/3, cost: 0.387256\n",
      "epoch   20/20 batch: 1/3, cost: 0.045676\n",
      "epoch   20/20 batch: 2/3, cost: 0.964546\n",
      "epoch   20/20 batch: 3/3, cost: 0.049074\n"
     ]
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "nb_epochs=20\n",
    "\n",
    "for epoch in range(nb_epochs+1):                                #epoch이 돌아갈 때마다 아래의 for문에서 dataloader가 호출이 되는데\n",
    "    for batch_idx, samples in enumerate(dataloader):            #2개 2개 1개 씩 호출이 된다.                                            \n",
    "        x_train, y_train= samples             #예시: x_train = [[73, 80, 75],    [93, 88, 93]]\n",
    "                                                    #y_train = [[152], [185]]\n",
    "\n",
    "        predictions=model(x_train)\n",
    "\n",
    "        cost=F.mse_loss(predictions, y_train)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        cost.backward() \n",
    "        optimizer.step()\n",
    "        \n",
    "        print('epoch {:4d}/{} batch: {}/{}, cost: {:.6f}'.format(epoch, nb_epochs, batch_idx+1, len(dataloader), cost.item()))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "batch가 1/3, 2/3, 3/3 이 되는 이유:\n",
    "\n",
    "총 데이터 개수는 5개입니다.<br>\n",
    "배치 크기는 2입니다.<br>\n",
    "5개를 2개씩 나누면 첫 번째와 두 번째 배치는 2개의 샘플로, 세 번째 배치는 나머지 1개로 처리됩니다.<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "notebook_kernel",
   "language": "python",
   "name": "notebook_kernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
