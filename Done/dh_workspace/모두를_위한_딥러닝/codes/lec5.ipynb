{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multivariate Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 여러개의 데이터를 받아보자.\n",
    "\n",
    "x_train=torch.FloatTensor([[73, 80, 75],\n",
    "                           [93,88,93],\n",
    "                           [89,91,80],\n",
    "                           [96,98,100],\n",
    "                           [73,66,70]])\n",
    "\n",
    "y_train=torch.FloatTensor([[152],[185],[180],[196],[142]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'W' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m#hypothesis를 구할 때 matmul을 써주자. 길이가 바뀌는 것과 상관없이 쓸 수 있다.\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m hypothesis\u001b[38;5;241m=\u001b[39mx_train\u001b[38;5;241m.\u001b[39mmatmul(W)\u001b[38;5;241m+\u001b[39mb\n",
      "\u001b[1;31mNameError\u001b[0m: name 'W' is not defined"
     ]
    }
   ],
   "source": [
    "#hypothesis를 구할 때 matmul을 써주자. 길이가 바뀌는 것과 상관없이 쓸 수 있다.\n",
    "hypothesis=x_train.matmul(W)+b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch    0/20 hypothesis: tensor([0., 0., 0., 0., 0.]), cost: 29661.800781\n",
      "epoch    1/20 hypothesis: tensor([67.2578, 80.8397, 79.6523, 86.7394, 61.6605]), cost: 9298.520508\n",
      "epoch    2/20 hypothesis: tensor([104.9128, 126.0990, 124.2466, 135.3015,  96.1821]), cost: 2915.712402\n",
      "epoch    3/20 hypothesis: tensor([125.9942, 151.4381, 149.2133, 162.4896, 115.5097]), cost: 915.040527\n",
      "epoch    4/20 hypothesis: tensor([137.7968, 165.6247, 163.1911, 177.7112, 126.3307]), cost: 287.936005\n",
      "epoch    5/20 hypothesis: tensor([144.4044, 173.5674, 171.0168, 186.2332, 132.3891]), cost: 91.371010\n",
      "epoch    6/20 hypothesis: tensor([148.1035, 178.0144, 175.3980, 191.0042, 135.7812]), cost: 29.758139\n",
      "epoch    7/20 hypothesis: tensor([150.1744, 180.5042, 177.8508, 193.6753, 137.6805]), cost: 10.445305\n",
      "epoch    8/20 hypothesis: tensor([151.3336, 181.8983, 179.2240, 195.1707, 138.7440]), cost: 4.391228\n",
      "epoch    9/20 hypothesis: tensor([151.9824, 182.6789, 179.9928, 196.0079, 139.3396]), cost: 2.493135\n",
      "epoch   10/20 hypothesis: tensor([152.3454, 183.1161, 180.4231, 196.4765, 139.6732]), cost: 1.897688\n",
      "epoch   11/20 hypothesis: tensor([152.5485, 183.3610, 180.6640, 196.7389, 139.8602]), cost: 1.710541\n",
      "epoch   12/20 hypothesis: tensor([152.6620, 183.4982, 180.7988, 196.8857, 139.9651]), cost: 1.651412\n",
      "epoch   13/20 hypothesis: tensor([152.7253, 183.5752, 180.8742, 196.9678, 140.0240]), cost: 1.632387\n",
      "epoch   14/20 hypothesis: tensor([152.7606, 183.6184, 180.9164, 197.0138, 140.0571]), cost: 1.625923\n",
      "epoch   15/20 hypothesis: tensor([152.7802, 183.6427, 180.9399, 197.0395, 140.0759]), cost: 1.623412\n",
      "epoch   16/20 hypothesis: tensor([152.7909, 183.6565, 180.9530, 197.0538, 140.0865]), cost: 1.622141\n",
      "epoch   17/20 hypothesis: tensor([152.7968, 183.6643, 180.9603, 197.0618, 140.0927]), cost: 1.621253\n",
      "epoch   18/20 hypothesis: tensor([152.7999, 183.6688, 180.9644, 197.0662, 140.0963]), cost: 1.620500\n",
      "epoch   19/20 hypothesis: tensor([152.8014, 183.6715, 180.9666, 197.0686, 140.0985]), cost: 1.619770\n"
     ]
    }
   ],
   "source": [
    "# 코드를 쭉 써보자.\n",
    "\n",
    "x_train=torch.FloatTensor([[73, 80, 75],\n",
    "                           [93,88,93],\n",
    "                           [89,91,90],\n",
    "                           [96,98,100],\n",
    "                           [73,66,70]])\n",
    "\n",
    "y_train=torch.FloatTensor([[152],[185],[180],[196],[142]])\n",
    "\n",
    "W=torch.zeros((3,1),requires_grad=True)\n",
    "b=torch.zeros(1,requires_grad=True)\n",
    "\n",
    "optimizer=torch.optim.SGD([W,b],lr=1e-5)\n",
    "\n",
    "nb_epochs=20\n",
    "for epoch in range(nb_epochs):\n",
    "    hypothesis=x_train.matmul(W)+b                          #regression한 값이다. ㅋㅋ\n",
    "\n",
    "    cost=torch.mean((hypothesis-y_train)**2)\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    cost.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    print('epoch {:4d}/{} hypothesis: {}, cost: {:.6f}'.format(epoch, nb_epochs, hypothesis.squeeze().detach(), cost.item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nn.Module (현재 nn.Module을 쓰는 이유는 W와 b를 자동으로 초기화 하고 계산하기 위해서이다.)\n",
    "\n",
    "# nn.Module을 사용하는 이유:\n",
    "# 파라미터 관리: nn.Module은 내부적으로 모델의 모든 파라미터(가중치, 편향 등)를 관리합니다. 이 파라미터들은 자동으로 .parameters() 메서드를 통해 추출할 수 있고, 학습 시 자동 미분을 지원합니다.\n",
    "# 모델 저장 및 로드: 모델을 nn.Module로 정의하면, 전체 모델을 파일로 저장하고 불러오는 작업이 매우 용이합니다.\n",
    "# 복잡한 네트워크 구조: nn.Module을 상속받으면 여러 개의 레이어를 손쉽게 조합하여 복잡한 네트워크를 구현할 수 있습니다.\n",
    "import torch.nn as nn\n",
    "\n",
    "class MultivariateLinearRegressionModel(nn.Module):     #nn.Module을 상속해서 인공신경망을 만드는 것을 피하게 할 수 있다.\n",
    "    def __init__(self):\n",
    "        super().__init__()              #super().__init__()이라는 코드가 다른 클래스의 속성 및 메소드를 자동으로 불러와 해당 클래스에서도 사용이 가능하도록 해준다.\n",
    "                                        #super()는 부모 클래스의 인스턴스를 반환하는 함수. __init__()는 부모 클래스의 초기화 메서드.\n",
    "        self.linear=nn.Linear(3,1)      #선형 계층(Linear Layer)을 정의. 입력차원은 3, 출력차원은 1\n",
    "                                        #self.linear는 이 선형 계층을 MultivariateLinearRegressionModel 클래스의 속성으로 저장        \n",
    "                                        #nn.Linear는 in_features(입력 특성의 수)와 out_features(출력 특성의 수)에 맞춰 W와 b를 자동으로 초기화\n",
    "\n",
    "    def forward(self,x):\n",
    "        return self.linear(x)           #self.linear는 nn.Linear(3, 1)로 정의되어 있기 때문에 W * x + b 연산이 자동으로 수행\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'torch.nn' has no attribute 'prediction'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[14], line 6\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# F.mse_loss\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m# pytorch는 다양한 cost function을 제공한다.\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfunctional\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mF\u001b[39;00m\n\u001b[1;32m----> 6\u001b[0m cost\u001b[38;5;241m=\u001b[39mF\u001b[38;5;241m.\u001b[39mmse_loss(nn\u001b[38;5;241m.\u001b[39mprediction, y_train)\n",
      "\u001b[1;31mAttributeError\u001b[0m: module 'torch.nn' has no attribute 'prediction'"
     ]
    }
   ],
   "source": [
    "# F.mse_loss\n",
    "# pytorch는 다양한 cost function을 제공한다.\n",
    "\n",
    "import torch.nn.functional as F\n",
    "\n",
    "cost=F.mse_loss(prediction, y_train)        #prediction: 모델의 출력값(예측값). 보통 모델을 통해 얻은 예측값입니다.\n",
    "                                            #y_train: 실제값(정답). 보통 훈련 데이터의 타겟 값입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch    0/20 hypothesis: tensor([49.7625, 66.0807, 61.7730, 68.2546, 51.2048]), cost: 12626.916992\n",
      "epoch    1/20 hypothesis: tensor([ 93.6441, 118.8224, 113.7406, 124.8460,  91.4333]), cost: 3959.014404\n",
      "epoch    2/20 hypothesis: tensor([118.2120, 148.3504, 142.8355, 156.5295, 113.9556]), cost: 1242.087524\n",
      "epoch    3/20 hypothesis: tensor([131.9668, 164.8819, 159.1247, 174.2679, 126.5648]), cost: 390.474457\n",
      "epoch    4/20 hypothesis: tensor([139.6678, 174.1372, 168.2445, 184.1991, 133.6241]), cost: 123.538574\n",
      "epoch    5/20 hypothesis: tensor([143.9795, 179.3187, 173.3503, 189.7591, 137.5762]), cost: 39.868130\n",
      "epoch    6/20 hypothesis: tensor([146.3937, 182.2196, 176.2090, 192.8721, 139.7887]), cost: 13.641304\n",
      "epoch    7/20 hypothesis: tensor([147.7455, 183.8435, 177.8095, 194.6149, 141.0273]), cost: 5.420306\n",
      "epoch    8/20 hypothesis: tensor([148.5024, 184.7526, 178.7056, 195.5907, 141.7205]), cost: 2.843035\n",
      "epoch    9/20 hypothesis: tensor([148.9264, 185.2614, 179.2074, 196.1370, 142.1085]), cost: 2.034797\n",
      "epoch   10/20 hypothesis: tensor([149.1640, 185.5462, 179.4884, 196.4429, 142.3255]), cost: 1.781040\n",
      "epoch   11/20 hypothesis: tensor([149.2972, 185.7055, 179.6458, 196.6142, 142.4469]), cost: 1.701088\n",
      "epoch   12/20 hypothesis: tensor([149.3719, 185.7946, 179.7339, 196.7101, 142.5147]), cost: 1.675635\n",
      "epoch   13/20 hypothesis: tensor([149.4139, 185.8443, 179.7833, 196.7639, 142.5526]), cost: 1.667246\n",
      "epoch   14/20 hypothesis: tensor([149.4377, 185.8720, 179.8110, 196.7940, 142.5736]), cost: 1.664224\n",
      "epoch   15/20 hypothesis: tensor([149.4511, 185.8874, 179.8266, 196.8109, 142.5852]), cost: 1.662859\n",
      "epoch   16/20 hypothesis: tensor([149.4588, 185.8959, 179.8354, 196.8204, 142.5916]), cost: 1.662029\n",
      "epoch   17/20 hypothesis: tensor([149.4633, 185.9005, 179.8404, 196.8257, 142.5950]), cost: 1.661367\n",
      "epoch   18/20 hypothesis: tensor([149.4660, 185.9030, 179.8432, 196.8287, 142.5967]), cost: 1.660752\n",
      "epoch   19/20 hypothesis: tensor([149.4677, 185.9043, 179.8449, 196.8304, 142.5975]), cost: 1.660159\n"
     ]
    }
   ],
   "source": [
    "# Full code\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class MultivariateLinearRegressionModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.linear=nn.Linear(3,1)\n",
    "\n",
    "    def forward(self,x):\n",
    "        return self.linear(x)               #참고로 x 크기는 (batch_size,3)\n",
    "\n",
    "x_train=torch.FloatTensor([[73, 80, 75],\n",
    "                           [93,88,93],\n",
    "                           [89,91,90],\n",
    "                           [96,98,100],\n",
    "                           [73,66,70]])\n",
    "\n",
    "y_train=torch.FloatTensor([[152],[185],[180],[196],[142]])\n",
    "\n",
    "model=MultivariateLinearRegressionModel()\n",
    "\n",
    "optimizer=torch.optim.SGD(model.parameters(),lr=1e-5)\n",
    "\n",
    "\n",
    "nb_epochs=20\n",
    "for epoch in range(nb_epochs):\n",
    "    hypothesis=model(x_train)\n",
    "    cost=F.mse_loss(hypothesis, y_train)\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    cost.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    print('epoch {:4d}/{} hypothesis: {}, cost: {:.6f}'.format(epoch, nb_epochs, hypothesis.squeeze().detach(), cost.item()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "notebook_kernel",
   "language": "python",
   "name": "notebook_kernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
